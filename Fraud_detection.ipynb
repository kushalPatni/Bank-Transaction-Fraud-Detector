{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71d76f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3538450e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset: (16426, 11)\n",
      "\n",
      "First 5 rows:\n",
      "   step      type      amount     nameOrig  oldbalanceOrg  newbalanceOrig  \\\n",
      "0   187   CASH_IN    46291.77   C229926567     2098868.16      2145159.92   \n",
      "1    42  CASH_OUT   149509.45   C156437847           0.00            0.00   \n",
      "2    98  CASH_OUT     8055.06  C1544350298        8055.06            0.00   \n",
      "3   550  TRANSFER   342309.91   C662184778      342309.91            0.00   \n",
      "4   586  CASH_OUT  2581549.92   C648614053     2581549.92            0.00   \n",
      "\n",
      "      nameDest  oldbalanceDest  newbalanceDest  isFraud  isFlaggedFraud  \n",
      "0   C851786608       532456.02       486164.25        0               0  \n",
      "1  C1660179148       183524.95       333034.40        0               0  \n",
      "2   C912405348            0.00         8055.06        1               0  \n",
      "3  C1740503020            0.00            0.00        1               0  \n",
      "4   C213455810            0.00      2581549.92        1               0  \n",
      "\n",
      "Columns after renaming: ['time_step', 'transaction_type', 'amount', 'origin_account', 'origin_old_balance', 'origin_new_balance', 'destination_account', 'destination_old_balance', 'destination_new_balance', 'is_fraud', 'is_flagged_fraud']\n",
      "\n",
      "Fraud vs Non-Fraud Counts:\n",
      "is_fraud\n",
      "0    8213\n",
      "1    8213\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Fraud percentage: 50.0000%\n",
      "\n",
      "Summary Statistics (numeric columns):\n",
      "          time_step        amount  origin_old_balance  origin_new_balance  \\\n",
      "count  16426.000000  1.642600e+04        1.642600e+04        1.642600e+04   \n",
      "mean     307.423353  8.248597e+05        1.247732e+06        5.297009e+05   \n",
      "std      193.242041  1.874735e+06        3.291152e+06        2.556663e+06   \n",
      "min        1.000000  0.000000e+00        0.000000e+00        0.000000e+00   \n",
      "25%      162.000000  3.694213e+04        1.063800e+04        0.000000e+00   \n",
      "50%      284.000000  1.719690e+05        1.181118e+05        0.000000e+00   \n",
      "75%      410.000000  5.434981e+05        8.006783e+05        0.000000e+00   \n",
      "max      743.000000  3.372047e+07        5.958504e+07        4.958504e+07   \n",
      "\n",
      "       destination_old_balance  destination_new_balance      is_fraud  \\\n",
      "count             1.642600e+04             1.642600e+04  16426.000000   \n",
      "mean              8.363819e+05             1.271853e+06      0.500000   \n",
      "std               3.549230e+06             4.078627e+06      0.500015   \n",
      "min               0.000000e+00             0.000000e+00      0.000000   \n",
      "25%               0.000000e+00             0.000000e+00      0.000000   \n",
      "50%               0.000000e+00             1.243661e+05      0.500000   \n",
      "75%               5.187456e+05             1.091814e+06      1.000000   \n",
      "max               2.362305e+08             2.367265e+08      1.000000   \n",
      "\n",
      "       is_flagged_fraud  \n",
      "count      16426.000000  \n",
      "mean           0.000974  \n",
      "std            0.031196  \n",
      "min            0.000000  \n",
      "25%            0.000000  \n",
      "50%            0.000000  \n",
      "75%            0.000000  \n",
      "max            1.000000  \n"
     ]
    }
   ],
   "source": [
    "# ===================== IMPORTS =====================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ===================== LOAD DATA =====================\n",
    "# Change path if needed\n",
    "file_path = \"AIML Dataset3.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(\"Shape of dataset:\", df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# ===================== RENAME COLUMNS =====================\n",
    "col_map = {\n",
    "    'step': 'time_step',\n",
    "    'type': 'transaction_type',\n",
    "    'amount': 'amount',\n",
    "    'nameOrig': 'origin_account',\n",
    "    'oldbalanceOrg': 'origin_old_balance',\n",
    "    'newbalanceOrig': 'origin_new_balance',\n",
    "    'nameDest': 'destination_account',\n",
    "    'oldbalanceDest': 'destination_old_balance',\n",
    "    'newbalanceDest': 'destination_new_balance',\n",
    "    'isFraud': 'is_fraud',\n",
    "    'isFlaggedFraud': 'is_flagged_fraud'\n",
    "}\n",
    "df.rename(columns={k: v for k, v in col_map.items() if k in df.columns}, inplace=True)\n",
    "\n",
    "print(\"\\nColumns after renaming:\", df.columns.tolist())\n",
    "\n",
    "# ===================== CLEAN NUMERIC COLUMNS =====================\n",
    "numeric_cols = [\n",
    "    \"amount\",\n",
    "    \"origin_old_balance\", \"origin_new_balance\",\n",
    "    \"destination_old_balance\", \"destination_new_balance\"\n",
    "]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")  # convert to numeric\n",
    "\n",
    "# Drop rows where key numeric values are missing\n",
    "df = df.dropna(subset=numeric_cols)\n",
    "\n",
    "# ===================== FRAUD COUNTS =====================\n",
    "fraud_counts = df['is_fraud'].value_counts()\n",
    "print(\"\\nFraud vs Non-Fraud Counts:\")\n",
    "print(fraud_counts)\n",
    "\n",
    "fraud_percentage = (fraud_counts[1] / fraud_counts.sum()) * 100\n",
    "print(\"\\nFraud percentage: {:.4f}%\".format(fraud_percentage))\n",
    "\n",
    "# ===================== DESCRIPTIVE STATS =====================\n",
    "print(\"\\nSummary Statistics (numeric columns):\")\n",
    "print(df.describe())\n",
    "\n",
    "# ===================== FEATURE ENGINEERING =====================\n",
    "df['balance_change_origin'] = df['origin_old_balance'] - df['origin_new_balance']\n",
    "df['balance_change_dest'] = df['destination_new_balance'] - df['destination_old_balance']\n",
    "df['transaction_hour'] = df['time_step'] % 24\n",
    "df['is_round_amount'] = (df['amount'] % 1000 == 0).astype(int)\n",
    "\n",
    "# ===================== VISUALIZATIONS =====================\n",
    "\n",
    "# Fraud vs Non-Fraud count\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='is_fraud', data=df, palette=['#4CAF50','#F44336'])\n",
    "plt.title(\"Fraud vs Non-Fraud Transactions\")\n",
    "plt.xlabel(\"Transaction Type (0 = Normal, 1 = Fraud)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Transaction amount distribution (log scale)\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(data=df, x='amount', hue='is_fraud', bins=100, log_scale=(True, False))\n",
    "plt.title(\"Transaction Amount Distribution (Log Scale)\")\n",
    "plt.show()\n",
    "\n",
    "# Fraud rate by transaction type\n",
    "if 'transaction_type' in df.columns:\n",
    "    plt.figure(figsize=(8,5))\n",
    "    fraud_rate = df.groupby('transaction_type')['is_fraud'].mean() * 100\n",
    "    sns.barplot(x=fraud_rate.index, y=fraud_rate.values, palette=\"Reds\")\n",
    "    plt.title(\"Fraud Rate by Transaction Type (%)\")\n",
    "    plt.ylabel(\"Fraud Rate %\")\n",
    "    plt.show()\n",
    "\n",
    "# Hourly fraud rate\n",
    "if 'time_step' in df.columns:\n",
    "    plt.figure(figsize=(10,5))\n",
    "    hourly = df.groupby(df['transaction_hour'])['is_fraud'].mean() * 100\n",
    "    sns.lineplot(x=hourly.index, y=hourly.values, marker='o')\n",
    "    plt.title(\"Fraud Rate by Hour of Day\")\n",
    "    plt.xlabel(\"Hour of Day\")\n",
    "    plt.ylabel(\"Fraud Rate (%)\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "789dac7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smita\\AppData\\Local\\Temp\\ipykernel_37288\\676172362.py:11: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"AIML Dataset.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (6362620, 11)\n",
      "\n",
      "Original dtypes:\n",
      "step                int64\n",
      "type               object\n",
      "amount            float64\n",
      "nameOrig           object\n",
      "oldbalanceOrg     float64\n",
      "newbalanceOrig    float64\n",
      "nameDest           object\n",
      "oldbalanceDest     object\n",
      "newbalanceDest    float64\n",
      "isFraud             int64\n",
      "isFlaggedFraud      int64\n",
      "dtype: object\n",
      "\n",
      "Conversion summary (columns where coercion produced NaNs):\n",
      " - oldbalanceDest: 1 values could not be converted and became NaN\n",
      "\n",
      "Dropped 1 rows due to non-convertible numeric values (see summary above).\n",
      "\n",
      "Dtypes after conversion/downcast:\n",
      "step                       int16\n",
      "type                    category\n",
      "amount                   float64\n",
      "nameOrig          string[python]\n",
      "oldbalanceOrg            float64\n",
      "newbalanceOrig           float64\n",
      "nameDest          string[python]\n",
      "oldbalanceDest           float64\n",
      "newbalanceDest           float64\n",
      "isFraud                     int8\n",
      "isFlaggedFraud              int8\n",
      "dtype: object\n",
      "\n",
      "Memory usage (MB): 1043.455451965332\n",
      "\n",
      "Class counts before balancing:\n",
      "isFraud\n",
      "0    6354406\n",
      "1       8213\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After Balancing class counts:\n",
      "isFraud\n",
      "0    8213\n",
      "1    8213\n",
      "Name: count, dtype: int64\n",
      "\n",
      "✅ Balanced dataset saved as 'AIML Dataset3.csv' (rows: 16426)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load dataset\n",
    "file_in = \"AIML Dataset.csv\"\n",
    "if not os.path.exists(file_in):\n",
    "    raise FileNotFoundError(f\"File not found: {file_in}\")\n",
    "\n",
    "df = pd.read_csv(\"AIML Dataset.csv\")\n",
    "print(\"Loaded:\", df.shape)\n",
    "print(\"\\nOriginal dtypes:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# --- Define desired column types (adjust these if your real column names differ) ---\n",
    "# Use: 'int', 'float', 'category', 'string' (or 'object' to keep as Python object)\n",
    "desired_types = {\n",
    "    'step': 'int',\n",
    "    'type': 'category',            # transaction type (categorical)\n",
    "    'amount': 'float',\n",
    "    'nameOrig': 'string',          # origin account id/name\n",
    "    'oldbalanceOrg': 'float',\n",
    "    'newbalanceOrig': 'float',\n",
    "    'nameDest': 'string',          # destination account id/name\n",
    "    'oldbalanceDest': 'float',\n",
    "    'newbalanceDest': 'float',\n",
    "    'isFraud': 'int',\n",
    "    'isFlaggedFraud': 'int'\n",
    "}\n",
    "\n",
    "# If your dataset column names differ (eg. isFraud vs is_fraud), map them here:\n",
    "# e.g., desired_types = {'step':'int', 'type':'category', 'amount':'float', 'is_fraud':'int', ...}\n",
    "\n",
    "# --- Convert columns safely ---\n",
    "df_copy = df.copy()\n",
    "conversion_errors = {}\n",
    "\n",
    "for col, kind in desired_types.items():\n",
    "    if col not in df_copy.columns:\n",
    "        print(f\"⚠️ Column '{col}' not found in dataset — skipping conversion for this column.\")\n",
    "        continue\n",
    "\n",
    "    if kind == 'int':\n",
    "        # convert to numeric then downcast to smallest integer\n",
    "        before_nonnull = df_copy[col].notna().sum()\n",
    "        df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce')\n",
    "        coerced = df_copy[col].isna().sum()\n",
    "        if coerced > 0:\n",
    "            conversion_errors[col] = coerced\n",
    "        # drop rows with NaN produced by coercion (we'll log how many)\n",
    "        # but don't drop immediately every column; postpone final drop to keep track of total failures\n",
    "    elif kind == 'float':\n",
    "        df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce')\n",
    "        coerced = df_copy[col].isna().sum()\n",
    "        if coerced > 0:\n",
    "            conversion_errors[col] = coerced\n",
    "    elif kind == 'category':\n",
    "        # treat as string then convert to category (handles missing)\n",
    "        df_copy[col] = df_copy[col].astype('string').fillna(\"UNKNOWN\")\n",
    "        df_copy[col] = df_copy[col].astype('category')\n",
    "    elif kind in ('string', 'object'):\n",
    "        df_copy[col] = df_copy[col].astype('string')\n",
    "    else:\n",
    "        print(f\"⚠️ Unknown desired type '{kind}' for column '{col}'\")\n",
    "\n",
    "# Report conversion problems\n",
    "if conversion_errors:\n",
    "    print(\"\\nConversion summary (columns where coercion produced NaNs):\")\n",
    "    for c, cnt in conversion_errors.items():\n",
    "        print(f\" - {c}: {cnt} values could not be converted and became NaN\")\n",
    "\n",
    "# Drop rows with any NaN in columns we converted to numeric (to keep dataset clean)\n",
    "numeric_cols = [c for c, t in desired_types.items() if t in ('int', 'float') and c in df_copy.columns]\n",
    "rows_before = len(df_copy)\n",
    "df_copy = df_copy.dropna(subset=numeric_cols)\n",
    "rows_after = len(df_copy)\n",
    "dropped = rows_before - rows_after\n",
    "if dropped > 0:\n",
    "    print(f\"\\nDropped {dropped} rows due to non-convertible numeric values (see summary above).\")\n",
    "else:\n",
    "    print(\"\\nNo rows dropped during numeric conversion.\")\n",
    "\n",
    "# Now downcast numeric columns to save memory\n",
    "for c in numeric_cols:\n",
    "    if c not in df_copy.columns:\n",
    "        continue\n",
    "    if desired_types[c] == 'int':\n",
    "        # convert floats that are actually integers\n",
    "        df_copy[c] = pd.to_numeric(df_copy[c], downcast='integer')\n",
    "    else:  # float\n",
    "        df_copy[c] = pd.to_numeric(df_copy[c], downcast='float')\n",
    "\n",
    "# Ensure integer columns are ints (no fractional parts after coercion)\n",
    "for c, t in desired_types.items():\n",
    "    if t == 'int' and c in df_copy.columns:\n",
    "        # If column is float (rare after downcast), convert safely to int\n",
    "        if pd.api.types.is_float_dtype(df_copy[c]):\n",
    "            if (df_copy[c] % 1 != 0).any():\n",
    "                print(f\"⚠️ Column '{c}' has non-integer values after conversion; keeping as float.\")\n",
    "            else:\n",
    "                df_copy[c] = df_copy[c].astype('int64')\n",
    "\n",
    "print(\"\\nDtypes after conversion/downcast:\")\n",
    "print(df_copy.dtypes)\n",
    "print(\"\\nMemory usage (MB):\", df_copy.memory_usage(deep=True).sum() / 1024**2)\n",
    "\n",
    "# --- Balance dataset via undersampling (your original logic) ---\n",
    "# If column name is 'isFraud' or 'is_fraud', ensure we use the correct one:\n",
    "target_col = None\n",
    "if 'isFraud' in df_copy.columns:\n",
    "    target_col = 'isFraud'\n",
    "elif 'is_fraud' in df_copy.columns:\n",
    "    target_col = 'is_fraud'\n",
    "elif 'isFraud' in df.columns:\n",
    "    target_col = 'isFraud'\n",
    "else:\n",
    "    raise KeyError(\"Target column 'isFraud' (or 'is_fraud') not found in dataset after conversion.\")\n",
    "\n",
    "fraud_df = df_copy[df_copy[target_col] == 1]\n",
    "nonfraud_df = df_copy[df_copy[target_col] == 0]\n",
    "\n",
    "print(\"\\nClass counts before balancing:\")\n",
    "print(df_copy[target_col].value_counts())\n",
    "\n",
    "# If fraud is the minority, undersample non-fraud to match fraud count.\n",
    "n_fraud = len(fraud_df)\n",
    "n_nonfraud = len(nonfraud_df)\n",
    "\n",
    "if n_fraud == 0:\n",
    "    raise ValueError(\"No fraud samples found in dataset! Cannot balance by undersampling.\")\n",
    "\n",
    "if n_nonfraud <= n_fraud:\n",
    "    print(\"Warning: Non-fraud class has less or equal samples than fraud. Skipping downsampling.\")\n",
    "    df_balanced = df_copy.copy()\n",
    "else:\n",
    "    nonfraud_downsampled = resample(\n",
    "        nonfraud_df,\n",
    "        replace=False,\n",
    "        n_samples=n_fraud,\n",
    "        random_state=42\n",
    "    )\n",
    "    df_balanced = pd.concat([fraud_df, nonfraud_downsampled], ignore_index=True)\n",
    "    df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nAfter Balancing class counts:\")\n",
    "print(df_balanced[target_col].value_counts())\n",
    "\n",
    "# Save to CSV\n",
    "out_file = \"AIML Dataset3.csv\"\n",
    "df_balanced.to_csv(out_file, index=False)\n",
    "print(f\"\\n✅ Balanced dataset saved as '{out_file}' (rows: {len(df_balanced)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e292e577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16426 entries, 0 to 16425\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   step            16426 non-null  int64  \n",
      " 1   type            16426 non-null  object \n",
      " 2   amount          16426 non-null  float64\n",
      " 3   oldbalanceOrg   16426 non-null  float64\n",
      " 4   newbalanceOrig  16426 non-null  float64\n",
      " 5   oldbalanceDest  16426 non-null  float64\n",
      " 6   newbalanceDest  16426 non-null  float64\n",
      "dtypes: float64(5), int64(1), object(1)\n",
      "memory usage: 898.4+ KB\n",
      "None\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93      1643\n",
      "           1       0.93      0.93      0.93      1643\n",
      "\n",
      "    accuracy                           0.93      3286\n",
      "   macro avg       0.93      0.93      0.93      3286\n",
      "weighted avg       0.93      0.93      0.93      3286\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1533  110]\n",
      " [ 120 1523]]\n",
      "\n",
      "ROC-AUC Score: 0.9832547308728559\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "# 1. Load dataset\n",
    "file_path = \"AIML Dataset3.csv\"   # change path if needed\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 2. Drop irrelevant columns (IDs)\n",
    "X = data.drop(columns=[\"isFraud\", \"nameOrig\", \"nameDest\", \"isFlaggedFraud\"])\n",
    "y = data[\"isFraud\"]\n",
    "\n",
    "print(X.info())\n",
    "\n",
    "# 3. Encode categorical column \"type\"\n",
    "X = pd.get_dummies(X, columns=[\"type\"], drop_first=True)\n",
    "\n",
    "# Save the feature columns order\n",
    "feature_columns = X.columns\n",
    "joblib.dump(feature_columns, \"feature_columns.pkl\")\n",
    "\n",
    "# 4. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 5. Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 6. Train Logistic Regression model\n",
    "log_reg = LogisticRegression(max_iter=1000, class_weight=\"balanced\", random_state=42)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 7. Predictions & evaluation\n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "y_prob = log_reg.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nROC-AUC Score:\", roc_auc_score(y_test, y_prob))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e4cacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.47      0.47      1643\n",
      "           1       0.47      0.47      0.47      1643\n",
      "\n",
      "    accuracy                           0.47      3286\n",
      "   macro avg       0.47      0.47      0.47      3286\n",
      "weighted avg       0.47      0.47      0.47      3286\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[775 868]\n",
      " [864 779]]\n",
      "\n",
      "ROC-AUC Score: 0.476254598623645\n"
     ]
    }
   ],
   "source": [
    "# Isolation Forest\n",
    "\n",
    "# 1. Load dataset\n",
    "file_path = \"AIML Dataset3.csv\"   # change path if needed\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 2. Drop irrelevant columns (IDs)\n",
    "X = data.drop(columns=[\"isFraud\", \"nameOrig\", \"nameDest\", \"isFlaggedFraud\"])\n",
    "y = data[\"isFraud\"]\n",
    "\n",
    "# 3. Encode categorical column \"type\"\n",
    "X = pd.get_dummies(X, columns=[\"type\"], drop_first=True)\n",
    "\n",
    "# 4. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 5. Scale features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 6. Train Isolation Forest\n",
    "contamination_rate = y_train.mean()  # fraction of fraud cases in training set\n",
    "iso_forest = IsolationForest(\n",
    "    contamination=contamination_rate, random_state=42\n",
    ")\n",
    "iso_forest.fit(X_train_scaled)\n",
    "\n",
    "# 7. Predictions\n",
    "y_pred_if = iso_forest.predict(X_test_scaled)\n",
    "\n",
    "# IsolationForest outputs: -1 = anomaly (fraud), 1 = normal\n",
    "# Convert to 0/1 labels (fraud=1, normal=0)\n",
    "y_pred_if = [1 if p == -1 else 0 for p in y_pred_if]\n",
    "\n",
    "# 8. Evaluation\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_if))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_if))\n",
    "\n",
    "# IsolationForest also gives anomaly scores (for ROC-AUC)\n",
    "y_scores = -iso_forest.decision_function(X_test_scaled)\n",
    "print(\"\\nROC-AUC Score:\", roc_auc_score(y_test, y_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20d0fc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16426 entries, 0 to 16425\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   step            16426 non-null  int64  \n",
      " 1   type            16426 non-null  object \n",
      " 2   amount          16426 non-null  float64\n",
      " 3   oldbalanceOrg   16426 non-null  float64\n",
      " 4   newbalanceOrig  16426 non-null  float64\n",
      " 5   oldbalanceDest  16426 non-null  float64\n",
      " 6   newbalanceDest  16426 non-null  float64\n",
      "dtypes: float64(5), int64(1), object(1)\n",
      "memory usage: 898.4+ KB\n",
      "None\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99      1643\n",
      "           1       0.99      1.00      0.99      1643\n",
      "\n",
      "    accuracy                           0.99      3286\n",
      "   macro avg       0.99      0.99      0.99      3286\n",
      "weighted avg       0.99      0.99      0.99      3286\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1619   24]\n",
      " [   1 1642]]\n",
      "\n",
      "ROC-AUC Score: 0.9994498877363491\n"
     ]
    }
   ],
   "source": [
    "#Random forest\n",
    "\n",
    "# 1. Load dataset\n",
    "file_path = \"AIML Dataset3.csv\"   # change path if needed\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 2. Drop irrelevant columns (IDs)\n",
    "X = data.drop(columns=[\"isFraud\", \"nameOrig\", \"nameDest\", \"isFlaggedFraud\"])\n",
    "y = data[\"isFraud\"]\n",
    "\n",
    "print(X.info())\n",
    "\n",
    "# 3. Encode categorical column \"type\"\n",
    "X = pd.get_dummies(X, columns=[\"type\"], drop_first=True)\n",
    "\n",
    "# 4. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 5. (Optional) Scale features – Random Forest doesn’t need scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "joblib.dump(X_train.columns.tolist(), \"feature_columns.pkl\")\n",
    "\n",
    "\n",
    "# 6. Train Random Forest model\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,        # number of trees\n",
    "    max_depth=None,          # let trees expand fully\n",
    "    class_weight=\"balanced\", # handle fraud imbalance\n",
    "    random_state=42,\n",
    "    n_jobs=-1                # use all CPU cores\n",
    ")\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 7. Predictions & evaluation\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "y_prob_rf = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_rf))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n",
    "print(\"\\nROC-AUC Score:\", roc_auc_score(y_test, y_prob_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97be5f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0000\n",
      "Testing Accuracy:  0.9933\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load dataset\n",
    "file_path = \"AIML Dataset.csv\"   # change path if needed\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 2. Drop irrelevant columns (IDs)\n",
    "X = data.drop(columns=[\"isFraud\", \"nameOrig\", \"nameDest\", \"isFlaggedFraud\"])\n",
    "y = data[\"isFraud\"]\n",
    "\n",
    "# 3. Encode categorical column \"type\"\n",
    "X = pd.get_dummies(X, columns=[\"type\"], drop_first=True)\n",
    "\n",
    "# 4. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 5. (Optional) Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 6. Train Random Forest\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=None,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 7. Training and Testing Accuracy\n",
    "train_acc = accuracy_score(y_train, rf_model.predict(X_train_scaled))\n",
    "test_acc = accuracy_score(y_test, rf_model.predict(X_test_scaled))\n",
    "\n",
    "print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Testing Accuracy:  {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "762aebd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Models saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Assuming you already trained these models earlier:\n",
    "# log_reg  -> Logistic Regression\n",
    "# iso_forest -> Isolation Forest\n",
    "# rf_model -> Random Forest\n",
    "\n",
    "# Save models\n",
    "joblib.dump(log_reg, \"logistic_regression_model.pkl\")\n",
    "joblib.dump(iso_forest, \"isolation_forest_model.pkl\")\n",
    "joblib.dump(rf_model, \"random_forest_model.pkl\")\n",
    "\n",
    "print(\"✅ Models saved successfully!\")\n",
    "\n",
    "# ----------------------------\n",
    "# To load the models later:\n",
    "# ----------------------------\n",
    "# log_reg_loaded = joblib.load(\"logistic_regression_model.pkl\")\n",
    "# iso_forest_loaded = joblib.load(\"isolation_forest_model.pkl\")\n",
    "# rf_model_loaded = joblib.load(\"random_forest_model.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ml_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
